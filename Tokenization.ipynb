{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics covered:\n",
    "- Tokenization\n",
    "    - word\n",
    "    - sentence\n",
    "- Text Normalization\n",
    "    - stemming\n",
    "    - lemmatization\n",
    "- StopWords\n",
    "- Name Entity Recognition\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization refers to break down the text into smaller units. It entails splitting paragraphs into sentences and sentences into words. It is one of the initial steps of any NLP pipeline.\n",
    "\n",
    "These tokens can be Words or Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few Terminology\n",
    "- Corpus : A large collection of sentences. (plural: Corpora)\n",
    "- Token : The smallest unit in corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')  # For tokenization\n",
    "\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog. Natural Language Processing allows machines to understand human language. Isn't that amazing? Nope We must try hard\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The quick brown fox jumps over the lazy dog.', 'Natural Language Processing allows machines to understand human language.', \"Isn't that amazing?\", 'Nope We must try hard']\n"
     ]
    }
   ],
   "source": [
    "sentences=sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'Natural', 'Language', 'Processing', 'allows', 'machines', 'to', 'understand', 'human', 'language', '.', 'Is', \"n't\", 'that', 'amazing', '?', 'Nope', 'We', 'must', 'try', 'hard']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words=word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤“ Fun Fact\n",
    "\n",
    "### Wordnet\n",
    "WordNet is a lexical database of semantic relations between words that links words into semantic relations including synonyms, hyponyms, and meronyms.\n",
    "***\n",
    "- Your Own Personal Dictionary. But A SMART one.\n",
    "- It let you know meaning of the word, synonymns, antonymns etc.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lexicon Entry for 'learning':\n",
      "the cognitive process of acquiring skill or knowledge\n"
     ]
    }
   ],
   "source": [
    "# Lexicon Example using WordNet\n",
    "from nltk.corpus import wordnet\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "syns = wordnet.synsets(\"learning\")\n",
    "print(\"\\nLexicon Entry for 'learning':\")\n",
    "print(syns[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fille', 'miss', 'missy', 'young_lady', 'young_woman'],\n",
       " ['female_child', 'little_girl'],\n",
       " ['daughter'],\n",
       " ['girlfriend', 'lady_friend'],\n",
       " []]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synonyms(\"girl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Stemming\n",
    "\n",
    "Stemming is the process of reducing a word to its root or base form. \n",
    "It may not result in an actual valid word.\n",
    "\n",
    "Example: \"running\", \"runs\", and \"ran\" may all be reduced to \"run\" or \"run\" could become \"runn\" using certain stemmers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "Lemmatization is also the process of reducing a word to its base form (lemma).\n",
    "However, it always returns a valid word, using a dictionary and part-of-speech (POS) tagging.\n",
    "Example: \"running\" becomes \"run\", \"better\" becomes \"good\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Then What's The difference? ðŸ¤”\n",
    "1. Stemming is faster but less accurate.\n",
    "2. Lemmatization is slower but more accurate and meaningful.\n",
    "3. Stemming may produce non-existent words; Lemmatization always returns real words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer    # stemming\n",
    "from nltk.stem import WordNetLemmatizer   #lemmatizing\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleText=\" When i am running, i feel better than before. Too much studying just not me.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemming Results:\n",
      "When --> when\n",
      "i --> i\n",
      "am --> am\n",
      "running --> run\n",
      ", --> ,\n",
      "i --> i\n",
      "feel --> feel\n",
      "better --> better\n",
      "than --> than\n",
      "before --> befor\n",
      ". --> .\n",
      "Too --> too\n",
      "much --> much\n",
      "studying --> studi\n",
      "just --> just\n",
      "not --> not\n",
      "me --> me\n",
      ". --> .\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(sampleText)\n",
    "print(\"\\nStemming Results:\")\n",
    "for word in words:\n",
    "    print(f\"{word} --> {stemmer.stem(word)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatization Results:\n",
      "When --> When\n",
      "i --> i\n",
      "am --> am\n",
      "running --> running\n",
      ", --> ,\n",
      "i --> i\n",
      "feel --> feel\n",
      "better --> better\n",
      "than --> than\n",
      "before --> before\n",
      ". --> .\n",
      "Too --> Too\n",
      "much --> much\n",
      "studying --> studying\n",
      "just --> just\n",
      "not --> not\n",
      "me --> me\n",
      ". --> .\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLemmatization Results:\")\n",
    "for word in words:\n",
    "    print(f\"{word} --> {lemmatizer.lemmatize(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Strong Examples Showing Differences:\n",
      "running --> Stem: run, Lemma: running\n",
      "flies --> Stem: fli, Lemma: fly\n",
      "better --> Stem: better, Lemma: better\n",
      "flying --> Stem: fli, Lemma: flying\n",
      "studies --> Stem: studi, Lemma: study\n",
      "feet --> Stem: feet, Lemma: foot\n",
      "ate --> Stem: ate, Lemma: ate\n"
     ]
    }
   ],
   "source": [
    "# Words that show difference between stemming and lemmatization\n",
    "print(\"\\nStrong Examples Showing Differences:\")\n",
    "sample_words = [\"running\", \"flies\", \"better\", \"flying\", \"studies\", \"feet\", \"ate\"]\n",
    "for word in sample_words:\n",
    "    stemmed = stemmer.stem(word)\n",
    "    lemmatized = lemmatizer.lemmatize(word)\n",
    "    print(f\"{word} --> Stem: {stemmed}, Lemma: {lemmatized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’¡ðŸ’¡ Brainy Bites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RegexpStemmer\n",
    "\n",
    "RegexpStemmer is a rule-based stemmer in NLTK that uses regular expressions to strip word suffixes. Unlike PorterStemmer or LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "get\n",
      "dis\n"
     ]
    }
   ],
   "source": [
    "# Applying RegexpStemmer\n",
    "\n",
    "from nltk.stem import RegexpStemmer\n",
    "R_stem=RegexpStemmer(\"ing$|s$|e$|able$\",min=4)\n",
    "arg=['eats','geting',\"disable\"]\n",
    "for i in arg:\n",
    "    print(R_stem.stem(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SnowBallStemmer\n",
    "Itâ€™s also called â€œPorter2â€ stemmer.\n",
    "More advanced than PorterStemmer with better linguistic rules.\n",
    "Supports multiple languages, unlike PorterStemmer (which only supports English)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eats.....eat\n",
      "geting.....gete\n",
      "disable.....disabl\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "Snowballstem=SnowballStemmer(\"english\")\n",
    "for  i in arg:\n",
    "    print(i+\".....\"+Snowballstem.stem(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Still Confused??ðŸ¥²\n",
    "### Here you go ðŸ˜Ž Full Comparison Table:\n",
    " | Method              | Key Feature                                                 | Returns Real Word? | Based on Rules or Dictionary? | Accuracy       | Language Support      |\n",
    " |---------------------|-------------------------------------------------------------|---------------------|-------------------------------|----------------|------------------------|\n",
    " | Porter Stemmer      | Oldest and simplest stemmer, chops suffixes                | âŒ (Not always)     | Rule-based                    | Low to Medium  | English only           |\n",
    " | Lemmatization       | Uses vocabulary + grammar (POS tagging) to get base word   | âœ… (Always)         | Dictionary-based              | High           | English (WordNet)      |\n",
    " | RegexpStemmer       | Applies custom regex rules (e.g., remove -ing, -ed, -s)    | âŒ (Usually not)    | Rule-based (regex)            | Low            | Depends on rules       |\n",
    "| Snowball Stemmer    | More advanced than Porter, improved linguistic logic       | âŒ (Not always)     | Rule-based (refined)          | Medium to High | Multiple languages     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "## Stop Words\n",
    "Stop words are common words that usually carry less meaning and are often removed from text data during preprocessing.\n",
    "Examples include: \"is\", \"and\", \"the\", \"a\", \"in\", etc.\n",
    "Removing them helps in focusing on more meaningful words in NLP tasks.\n",
    "\n",
    "**How it removing them helps?**\n",
    "- They occur frequently but add little information.\n",
    "- Reducing them simplifies the dataset and improves model performance.\n",
    "- Eliminating them reduces noise in the data.\n",
    "- It helps in decreasing the size of the dataset.\n",
    "- Enhances the efficiency of text classification and search-related tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Required Libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Words:\n",
      "['When', 'i', 'am', 'running', ',', 'i', 'feel', 'better', 'than', 'before', '.', 'Too', 'much', 'studying', 'just', 'not', 'me', '.']\n",
      "\n",
      "Filtered Words (Stop Words Removed):\n",
      "['running', ',', 'feel', 'better', '.', 'much', 'studying', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "words = word_tokenize(sampleText)\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english')) # setting the language\n",
    "\n",
    "# Filter out stop words\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print(\"\\nOriginal Words:\")\n",
    "print(words)\n",
    "\n",
    "print(\"\\nFiltered Words (Stop Words Removed):\")\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is NER?**\n",
    "\n",
    "Named Entity Recognition (NER) is the process of locating and classifying named entities in text into predefined categories such as:\n",
    "- Person names\n",
    "- Organizations\n",
    "- Locations\n",
    "- Dates\n",
    "- Time expressions\n",
    "- Monetary values\n",
    "\n",
    "NER helps in understanding the meaning of text by identifying important nouns that have a specific meaning or role.\n",
    "\n",
    "### Why is NER important?\n",
    "- Helps extract structured information from unstructured text.\n",
    "- Useful in applications like chatbots, search engines, recommendation systems, and summarization.\n",
    "- Makes information retrieval more accurate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple - ORG\n",
      "Steve Jobs - PERSON\n",
      "Cupertino - GPE\n",
      "1976 - DATE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nApple - ORG\\nSteve Jobs - PERSON\\nCupertino - GPE\\n1976 - DATE\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Apple was founded by Steve Jobs in Cupertino in 1976.\"\n",
    "# Try your own!\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print named entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"-\", ent.label_)\n",
    "\n",
    "\n",
    "### Output:\n",
    "\"\"\"\n",
    "Apple - ORG\n",
    "Steve Jobs - PERSON\n",
    "Cupertino - GPE\n",
    "1976 - DATE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Test Your Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "# ðŸ¥³ðŸ¥³ Congrats Everyone we did it!!\n",
    "Now you know a lot more than before you.\n",
    "### You should pat your back. Coz i certainly am.\n",
    "\n",
    "Thankyou for being a part of it. \n",
    "Happy Creative Coding!!! \n",
    "# ðŸ¤˜ðŸ¤˜\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
